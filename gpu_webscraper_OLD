#!/usr/bin/env python3
"""
Universal eBay Price Scraper
Based on the working GPU scraper, now supports any item type
Supports Excel/JSON input and output formats
"""

import random
import requests
from bs4 import BeautifulSoup
import json
import numpy as np
import pandas as pd
import time
import re
import os
from urllib.parse import quote
from statistics import median
from datetime import datetime

# ========================
# CONFIGURATION
# ========================
REQUEST_DELAY_MIN = 3  # Minimum seconds between requests
REQUEST_DELAY_MAX = 8  # Maximum seconds between requests
DEFAULT_PRICE_RANGE = (5, 5000)  # Default min/max prices
MAX_REQUESTS_PER_SESSION = 5  # Reset session after this many requests
RETRY_ATTEMPTS = 3  # Number of retry attempts for failed requests

# Rotate through different user agents
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

# Common exclusion filters (can be customized)
DEFAULT_EXCLUDE_KEYWORDS = [
    "for parts", "spares", "faulty", "not working", "broken", "bundle", 
    "system", "lot", "job lot", "combo", "damaged", "repair", "untested"
]
USER_FILTERS_FILE = "user_filters.json"

def load_user_filters():
    if os.path.exists(USER_FILTERS_FILE):
        try:
            with open(USER_FILTERS_FILE, "r", encoding="utf-8") as f:
                user_filters = json.load(f)
            if isinstance(user_filters, dict):
                print(f"‚úÖ Loaded {len(user_filters)} user-defined filters from {USER_FILTERS_FILE}")
                return user_filters
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load user filters: {e}")
    return {}


    # Merge user filters into global FILTER_PROFILES
def merge_filters(default_profiles):
    user_filters = load_user_filters()
    merged = default_profiles.copy()
    for key, keywords in user_filters.items():
        if isinstance(keywords, list):
            merged[key] = keywords
    return merged

# Predefined filter profiles for different item types
FILTER_PROFILES = {
    "graphics_card": [
        # Condition filters
        "for parts", "spares", "faulty", "not working", "broken", "damaged", 
        "repair", "untested", "defective", "dead", "burnt", "fried",
        
        # Bundle/system filters
        "bundle", "system", "pc", "computer", "build", "rig", "setup",
        "lot", "job lot", "bulk", "wholesale", "multiple", "x2", "x3", "x4",
        
        # Mining related (often worn out)
        "mining", "miner", "crypto", "bitcoin", "ethereum", "used for mining",
        "mining rig", "farm", "24/7", "heavy use",
        
        # Parts/incomplete
        "cooler only", "heatsink", "fan", "bracket", "box only", "manual only",
        "screws", "cable", "adapter",
        
        # Modified/custom
        "modified", "custom", "painted", "water cooled", "liquid cooled",
        "bios mod", "flashed", "custom firmware"
    ],
    
    "laptop": [
        "for parts", "spares", "faulty", "not working", "broken", "damaged",
        "cracked screen", "dead", "won't boot", "motherboard issue",
        "lot", "job lot", "bulk", "wholesale", "multiple",
        "battery dead", "no charger", "missing keys", "liquid damage"
    ],
    
    "phone": [
        "for parts", "spares", "faulty", "not working", "broken", "damaged",
        "cracked", "smashed", "water damage", "icloud locked", "blacklisted",
        "lot", "job lot", "bulk", "wholesale", "multiple",
        "no power", "dead", "motherboard", "logic board"
    ],
    
    "camera": [
        "for parts", "spares", "faulty", "not working", "broken", "damaged",
        "shutter issue", "mirror stuck", "lens error", "dead",
        "lot", "job lot", "bulk", "wholesale", "multiple",
        "body only", "lens only", "no lens", "no battery", "no charger"
    ],
    
    "console": [
        "for parts", "spares", "faulty", "not working", "broken", "damaged",
        "red ring", "ylod", "disc drive", "overheating", "dead",
        "lot", "job lot", "bulk", "wholesale", "multiple", "bundle",
        "no controller", "no cables", "console only"
    ],
    
    "car_parts": [
        "for parts", "salvage", "scrap", "damaged", "broken", "cracked",
        "rusted", "corroded", "worn", "used condition", "needs repair",
        "lot", "job lot", "bulk", "wholesale", "multiple"
    ],
    
    "clothing": [
        "damaged", "stained", "torn", "ripped", "holes", "faded",
        "lot", "job lot", "bulk", "wholesale", "bundle",
        "replica", "fake", "copy", "knockoff"
    ]
}


class UniversalPriceScraper:
    def __init__(self, delay=None, exclude_keywords=None, price_range=None, filter_profile=None):
        # Anti-detection settings
        self.delay_min = REQUEST_DELAY_MIN
        self.delay_max = REQUEST_DELAY_MAX
        self.price_range = price_range or DEFAULT_PRICE_RANGE
        self.session = None
        self.request_count = 0
        self.session_requests = 0
        self.last_request_time = 0
        
        # Set up exclusion keywords
        if exclude_keywords is not None:
            self.exclude_keywords = exclude_keywords
        elif filter_profile and filter_profile in FILTER_PROFILES:
            self.exclude_keywords = FILTER_PROFILES[filter_profile]
            print(f"üìã Using '{filter_profile}' filter profile ({len(self.exclude_keywords)} exclusions)")
        else:
            self.exclude_keywords = DEFAULT_EXCLUDE_KEYWORDS
        
        self.filter_profile = filter_profile
        
        # Initialize first session
        self._create_new_session()
        
    def _create_new_session(self):
        """Create a new session with random user agent and headers for anti-detection"""
        self.session = requests.Session()
        
        # Random user agent
        user_agent = random.choice(USER_AGENTS)
        
        # Realistic headers that vary slightly
        headers = {
            'User-Agent': user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice(['en-GB,en;q=0.9,en-US;q=0.8', 'en-US,en;q=0.9', 'en-GB,en;q=0.8']),
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': random.choice(['none', 'same-origin', 'cross-site']),
            'Cache-Control': random.choice(['no-cache', 'max-age=0']),
            'DNT': random.choice(['1', '0']),
        }
        
        # Sometimes add additional headers
        if random.random() > 0.5:
            headers['Sec-CH-UA'] = '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"'
            headers['Sec-CH-UA-Mobile'] = '?0'
            headers['Sec-CH-UA-Platform'] = f'"{random.choice(["Windows", "macOS", "Linux"])}"'
        
        self.session.headers.update(headers)
        self.session_requests = 0
        
        print(f"üîÑ New session created (User-Agent: {user_agent[:50]}...)")
    
    def _smart_delay(self):
        """Implement smart delay with randomization to avoid detection"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        # Base delay with randomization
        base_delay = random.uniform(self.delay_min, self.delay_max)
        
        # Add extra delay if making requests too quickly
        if time_since_last < 2:
            extra_delay = random.uniform(2, 5)
            print(f"   ‚è≥ Quick succession detected, adding {extra_delay:.1f}s delay")
            base_delay += extra_delay
        
        # Progressive delay - longer delays as we make more requests
        if self.request_count > 10:
            progressive_delay = (self.request_count - 10) * 0.5
            base_delay += progressive_delay
            if progressive_delay > 1:
                print(f"   ‚è≥ Progressive delay: +{progressive_delay:.1f}s")
        
        print(f"   ‚è±Ô∏è Waiting {base_delay:.1f} seconds...")
        time.sleep(base_delay)
        self.last_request_time = time.time()
    
    def _make_safe_request(self, url, max_retries=RETRY_ATTEMPTS):
        """Make a request with anti-detection measures"""
        
        # Create new session if needed
        if self.session is None or self.session_requests >= MAX_REQUESTS_PER_SESSION:
            self._create_new_session()
        
        # Smart delay before request (except first request)
        if self.request_count > 0:
            self._smart_delay()
        
        for attempt in range(max_retries):
            try:
                print(f"   üì° Making request (attempt {attempt + 1}/{max_retries})")
                
                response = self.session.get(url, timeout=30)
                self.request_count += 1
                self.session_requests += 1
                
                # Check for anti-bot responses
                if response.status_code == 200:
                    # Check for eBay's anti-bot page content
                    response_text_lower = response.text.lower()
                    if any(phrase in response_text_lower for phrase in [
                        'blocked', 'captcha', 'security check', 'suspicious activity',
                        'temporarily unavailable', 'access denied', 'spam'
                    ]):
                        print(f"   üö´ Anti-bot detection triggered (attempt {attempt + 1})")
                        if attempt < max_retries - 1:
                            wait_time = 30 + attempt * 15
                            print(f"   ‚è≥ Waiting {wait_time}s and creating new session...")
                            time.sleep(wait_time)
                            self._create_new_session()
                            continue
                        else:
                            print(f"   ‚ùå All attempts failed - blocked by eBay")
                            return None
                    else:
                        print(f"   ‚úÖ Success! ({len(response.content)} bytes)")
                        return response
                
                elif response.status_code == 403:
                    print(f"   üö´ 403 Forbidden - likely blocked (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        wait_time = 60 + attempt * 30
                        print(f"   ‚è≥ Waiting {wait_time}s and creating new session...")
                        time.sleep(wait_time)
                        self._create_new_session()
                        continue
                
                elif response.status_code == 429:
                    print(f"   ‚è≥ 429 Rate Limited (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) * 30
                        print(f"   ‚è≥ Exponential backoff: {wait_time}s")
                        time.sleep(wait_time)
                        self._create_new_session()
                        continue
                
                else:
                    print(f"   ‚ùå Status {response.status_code} (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(10 + attempt * 5)
                        continue
                
            except requests.exceptions.Timeout:
                print(f"   ‚è∞ Timeout (attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    time.sleep(15)
                    continue
            
            except requests.exceptions.RequestException as e:
                print(f"   ‚ùå Request error: {e} (attempt {attempt + 1})")
                if attempt < max_retries - 1:
                    time.sleep(10)
                    continue
        
        print(f"   ‚ùå All {max_retries} attempts failed")
        return None
    
    def extract_prices_from_html(self, html_content, query_terms):
        """Extract prices from eBay HTML using pattern matching with better error handling"""
        prices = []
        excluded_count = 0
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Check if we got a proper eBay page
            page_text = soup.get_text()[:1000].lower()
            if 'ebay' not in page_text:
                print("   ‚ö†Ô∏è Warning: Page doesn't appear to be eBay")
                return []
            
            # Check for anti-bot content
            if any(phrase in page_text for phrase in ['blocked', 'captcha', 'security check']):
                print("   üö´ Anti-bot page detected in HTML content")
                return []
            
            # Find all price patterns
            price_pattern = r'¬£([\d,]+\.?\d*)'
            all_prices = re.findall(price_pattern, html_content)
            
            print(f"   Found {len(all_prices)} total price patterns")
            
            # Get all text and split into sections
            all_text = soup.get_text()
            query_words = [word.lower() for word in query_terms.split() if len(word) > 2]
            
            # Find relevant sections containing our search terms
            relevant_sections = []
            lines = all_text.split('\n')
            
            for i, line in enumerate(lines):
                line_lower = line.lower().strip()
                if any(word in line_lower for word in query_words) and len(line_lower) > 5:
                    # Get surrounding context
                    start = max(0, i-3)
                    end = min(len(lines), i+4)
                    section = ' '.join(lines[start:end])
                    relevant_sections.append(section)
            
            print(f"   Found {len(relevant_sections)} relevant sections")
            
            # Extract prices from relevant sections with filtering
            section_prices = []
            for section in relevant_sections:
                section_lower = section.lower()
                
                # Check for exclusion keywords
                excluded_reasons = []
                for keyword in self.exclude_keywords:
                    if keyword.lower() in section_lower:
                        excluded_reasons.append(keyword)
                
                if excluded_reasons:
                    excluded_count += 1
                    if excluded_count <= 3:  # Only show first few to avoid spam
                        print(f"   ‚ùå Excluded listing: {excluded_reasons[0]}")
                    continue
                
                # Find prices in this section
                section_price_matches = re.findall(price_pattern, section)
                for price_str in section_price_matches:
                    try:
                        price = float(price_str.replace(',', ''))
                        if self.price_range[0] <= price <= self.price_range[1]:
                            section_prices.append(price)
                    except ValueError:
                        continue
            
            print(f"   Found {len(section_prices)} valid prices after filtering")
            if excluded_count > 0:
                print(f"   üö´ Excluded {excluded_count} listings due to filter keywords")
            
            # If no contextual prices, fall back to all prices in range
            if section_prices:
                prices = section_prices
            else:
                for price_str in all_prices:
                    try:
                        price = float(price_str.replace(',', ''))
                        if self.price_range[0] <= price <= self.price_range[1]:
                            prices.append(price)
                    except ValueError:
                        continue
            
            # Remove obvious duplicates (UI elements)
            if len(prices) > 10:
                price_counts = {}
                for price in prices:
                    price_counts[price] = price_counts.get(price, 0) + 1
                
                # Remove prices appearing too frequently
                max_allowed = len(prices) * 0.3
                filtered_prices = [price for price, count in price_counts.items() 
                                 if count <= max_allowed]
                if len(filtered_prices) >= 3:
                    prices = filtered_prices
            
            return prices
            
        except Exception as e:
            print(f"   ‚ùå Error extracting prices: {e}")
            return []
    
    def scrape_item_prices(self, query, max_results=50, category=None):
        """Scrape eBay sold listings with anti-detection measures"""
        print(f"üîç Searching for: '{query}'")
        print(f"üî¢ Total requests made so far: {self.request_count}")
        
        all_prices = []
        
        # Build search parameters
        params = {
            '_nkw': query,
            'LH_Sold': '1',     # Sold listings only
            'LH_Complete': '1', # Completed listings
            '_ipg': '240',      # Max items per page
            'rt': 'nc',
            '_udlo': str(self.price_range[0]),  # Min price
            '_udhi': str(self.price_range[1]),  # Max price
            '_sop': '13',       # Sort by: time ending soonest
        }
        
        # Add category if specified
        if category:
            params['_sacat'] = str(category)
        
        # Try different URL formats (eBay.com sometimes works when .co.uk is blocked)
        base_urls = [
            "https://www.ebay.co.uk/sch/i.html",
            "https://www.ebay.com/sch/i.html",  # Fallback to US site
        ]
        
        for base_url in base_urls:
            try:
                # Build URL with some randomization to avoid detection
                param_list = []
                for k, v in params.items():
                    param_list.append(f"{k}={quote(str(v))}")
                
                # Sometimes shuffle parameter order (minor obfuscation)
                if random.random() > 0.5:
                    random.shuffle(param_list)
                
                param_string = "&".join(param_list)
                full_url = f"{base_url}?{param_string}"
                
                print(f"   üåê Trying: {base_url}")
                
                # Make the request with anti-detection
                response = self._make_safe_request(full_url)
                
                if not response:
                    print(f"   ‚ùå Failed to get response from {base_url}")
                    continue
                
                # Extract prices
                prices = self.extract_prices_from_html(response.text, query)
                
                if prices:
                    all_prices.extend(prices)
                    print(f"   ‚úÖ Found {len(prices)} prices")
                    break  # Success!
                else:
                    print(f"   ‚ö†Ô∏è No prices found from {base_url}")
                
            except Exception as e:
                print(f"   ‚ùå Error with {base_url}: {e}")
                continue
        
        # Remove duplicates
        unique_prices = list(set(all_prices))
        
        # Limit results
        if len(unique_prices) > max_results:
            unique_prices = sorted(unique_prices)[:max_results]
        
        print(f"‚úÖ Final result: {len(unique_prices)} unique prices")
        if unique_prices:
            print(f"   Range: ¬£{min(unique_prices):.2f} - ¬£{max(unique_prices):.2f}")
            print(f"   Sample: {unique_prices[:5]}")
        
        return unique_prices
    
    def filter_outliers(self, prices):
        """Remove outliers using IQR method"""
        if len(prices) < 3:
            mean_price = np.mean(prices) if prices else np.nan
            return prices, mean_price, np.nan
        
        prices = np.array(prices)
        
        # IQR method
        q1, q3 = np.percentile(prices, [25, 75])
        iqr = q3 - q1
        lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
        iqr_filtered = prices[(prices >= lower) & (prices <= upper)]
        
        # If too many removed, use median absolute deviation
        if len(iqr_filtered) < len(prices) * 0.6:
            median_price = np.median(prices)
            mad = np.median(np.abs(prices - median_price))
            mad_lower = median_price - 3 * mad
            mad_upper = median_price + 3 * mad
            filtered = prices[(prices >= mad_lower) & (prices <= mad_upper)]
        else:
            filtered = iqr_filtered
        
        # Calculate trimmed mean
        if len(filtered) >= 6:
            sorted_filtered = np.sort(filtered)
            trim_count = max(1, int(len(sorted_filtered) * 0.1))
            trimmed = sorted_filtered[trim_count:-trim_count]
        else:
            trimmed = filtered
        
        mean_price = np.mean(trimmed) if len(trimmed) > 0 else np.nan
        std_dev = np.std(filtered) if len(filtered) > 0 else np.nan
        
        return filtered.tolist(), mean_price, std_dev

def load_items_from_file(file_path):
    """Load items from Excel or JSON file with filter support"""
    if not os.path.exists(file_path):
        print(f"‚ùå File not found: {file_path}")
        return []
    
    file_ext = os.path.splitext(file_path)[1].lower()
    items = []
    
    try:
        if file_ext in ['.xlsx', '.xls']:
            # Load Excel file
            df = pd.read_excel(file_path)
            print(f"üìä Loaded Excel file with {len(df)} rows")
            print(f"   Columns: {list(df.columns)}")
            
            # Try to identify relevant columns
            item_col = None
            category_col = None
            filter_col = None
            price_range_col = None
            
            for col in df.columns:
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in ['item', 'product', 'name', 'gpu', 'search']):
                    item_col = col
                elif any(keyword in col_lower for keyword in ['category', 'cat', 'type']):
                    category_col = col
                elif any(keyword in col_lower for keyword in ['filter', 'profile', 'exclusion']):
                    filter_col = col
                elif any(keyword in col_lower for keyword in ['price', 'range', 'min', 'max']):
                    price_range_col = col
            
            if not item_col:
                item_col = df.columns[0]  # Use first column as item name
                print(f"   Using '{item_col}' as item column")
            
            # Convert to list of dictionaries
            for _, row in df.iterrows():
                item_name = str(row[item_col]).strip()
                if item_name and item_name.lower() not in ['nan', 'none', '']:
                    item_data = {
                        'name': item_name,
                        'category': str(row[category_col]).strip() if category_col and not pd.isna(row[category_col]) else None,
                        'filter_profile': str(row[filter_col]).strip() if filter_col and not pd.isna(row[filter_col]) else None,
                        'original_data': dict(row)  # Keep original data for output
                    }
                    items.append(item_data)
        
        elif file_ext == '.json':
            # Load JSON file
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                # List of items
                for item in data:
                    if isinstance(item, dict):
                        # Look for name/item field
                        name = (item.get('name') or item.get('item') or 
                               item.get('product') or item.get('search_term'))
                        if name:
                            items.append({
                                'name': str(name).strip(),
                                'category': item.get('category'),
                                'filter_profile': item.get('filter_profile') or item.get('filter'),
                                'original_data': item
                            })
                    elif isinstance(item, str):
                        # Simple list of item names
                        items.append({
                            'name': item.strip(), 
                            'original_data': {'name': item}
                        })
            
            elif isinstance(data, dict):
                # Dictionary format
                if 'items' in data:
                    # Process nested items list
                    nested_items = data['items']
                    if isinstance(nested_items, list):
                        for item in nested_items:
                            if isinstance(item, dict):
                                name = (item.get('name') or item.get('item') or 
                                       item.get('product') or item.get('product_name') or
                                       item.get('search_term'))
                                if name:
                                    items.append({
                                        'name': str(name).strip(),
                                        'category': item.get('category') or item.get('ebay_category'),
                                        'filter_profile': item.get('filter_profile') or item.get('filter'),
                                        'original_data': item
                                    })
                            elif isinstance(item, str):
                                items.append({
                                    'name': item.strip(),
                                    'original_data': {'name': item}
                                })
                else:
                    # Single item
                    name = data.get('name') or data.get('item') or data.get('product')
                    if name:
                        items.append({
                            'name': str(name).strip(),
                            'category': data.get('category'),
                            'filter_profile': data.get('filter_profile') or data.get('filter'),
                            'original_data': data
                        })
        
        print(f"‚úÖ Loaded {len(items)} items from {file_path}")
        return items
        
    except Exception as e:
        print(f"‚ùå Error loading file: {e}")
        return []

def save_results_with_template(results, template_file, output_file):
    try:
        df_template = pd.read_excel(template_file)
        print(f"üìë Loaded template with {df_template.shape[0]} rows and {df_template.shape[1]} cols")


        # Build results DataFrame
        results_df = pd.DataFrame(results)


        # Try to map based on column names in template
        mapped_df = df_template.copy()
        for col in mapped_df.columns:
            col_lower = col.lower()
            if "item" in col_lower and "name" in results_df:
                mapped_df[col] = results_df.get("item_name")
            elif "average" in col_lower and "average_price" in results_df:
                mapped_df[col] = results_df.get("average_price")
            elif "median" in col_lower and "median_price" in results_df:
                mapped_df[col] = results_df.get("median_price")
            elif "count" in col_lower and "sample_count" in results_df:
                mapped_df[col] = results_df.get("sample_count")
            elif "range" in col_lower and "price_range" in results_df:
                mapped_df[col] = results_df.get("price_range")
            elif "std" in col_lower and "standard_deviation" in results_df:
                mapped_df[col] = results_df.get("standard_deviation")


# Save output
        mapped_df.to_excel(output_file, index=False)
        print(f"‚úÖ Results exported into template: {output_file}")
    except Exception as e:
        print(f"‚ùå Error using template export: {e}")


# === MODIFICATION in save_results ===
def save_results(results, output_file, original_format='json', template_file=None):
    """Save results to JSON, Excel, or template-based Excel"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")


    if template_file:
        save_results_with_template(results, template_file, output_file)
        return

def interactive_mode():
    """Interactive mode for single item searches"""
    print("\nüîç INTERACTIVE SEARCH MODE")
    print("=" * 40)
    
    while True:
        # Get search term
        search_term = input("\nEnter item to search for (or 'quit' to exit): ").strip()
        
        if search_term.lower() in ['quit', 'exit', 'q']:
            break
        
        if not search_term:
            print("Please enter a search term.")
            continue
        
        # Show available filter profiles
        print(f"\nüìã Available filter profiles:")
        print("0. Default (basic filtering)")
        for i, (profile_name, keywords) in enumerate(FILTER_PROFILES.items(), 1):
            print(f"{i}. {profile_name} ({len(keywords)} exclusions)")
        print(f"{len(FILTER_PROFILES)+1}. custom (enter your own keywords)")
        
        # Get filter choice
        try:
            filter_choice = input(f"\nChoose filter profile (0-{len(FILTER_PROFILES)+1}): ").strip()
            
            if filter_choice == '0' or filter_choice == '':
                filter_profile = None
                exclude_keywords = DEFAULT_EXCLUDE_KEYWORDS
            elif filter_choice.isdigit() and 1 <= int(filter_choice) <= len(FILTER_PROFILES):
                profile_name = list(FILTER_PROFILES.keys())[int(filter_choice)-1]
                filter_profile = profile_name
                exclude_keywords = None
                print(f"‚úÖ Using '{profile_name}' filter profile")
            elif filter_choice == str(len(FILTER_PROFILES)+1):
                # Custom keywords
                custom_input = input("Enter exclusion keywords (comma-separated): ").strip()
                if custom_input:
                    exclude_keywords = [kw.strip().lower() for kw in custom_input.split(',')]
                    filter_profile = None
                    print(f"‚úÖ Using custom filters: {exclude_keywords}")
                else:
                    exclude_keywords = DEFAULT_EXCLUDE_KEYWORDS
                    filter_profile = None
            else:
                print("Invalid choice, using default filters")
                exclude_keywords = DEFAULT_EXCLUDE_KEYWORDS
                filter_profile = None
        except:
            exclude_keywords = DEFAULT_EXCLUDE_KEYWORDS
            filter_profile = None
        
        # Optional category
        category = input("Enter eBay category ID (optional, press Enter to skip): ").strip()
        category = category if category.isdigit() else None
        
        # Optional price range
        price_input = input("Enter price range as 'min,max' (optional, press Enter for default): ").strip()
        price_range = DEFAULT_PRICE_RANGE
        if price_input and ',' in price_input:
            try:
                min_price, max_price = map(float, price_input.split(','))
                price_range = (min_price, max_price)
            except ValueError:
                print("Invalid price range format, using default.")
        
        # Create scraper with selected options
        scraper = UniversalPriceScraper(
            exclude_keywords=exclude_keywords,
            filter_profile=filter_profile,
            price_range=price_range
        )
        
        print(f"\nüöÄ Searching for: {search_term}")
        print(f"üí∞ Price range: ¬£{price_range[0]}-¬£{price_range[1]}")
        if filter_profile:
            print(f"üö´ Filter profile: {filter_profile}")
        else:
            print(f"üö´ Exclusions: {len(exclude_keywords)} keywords")
        print("-" * 50)
        
        # Scrape prices
        prices = scraper.scrape_item_prices(search_term, category=category)
        
        if not prices:
            print("‚ùå No prices found.")
            continue
        
        # Process results
        filtered_prices, avg_price, std_dev = scraper.filter_outliers(prices)
        
        # Display results
        print(f"\nüìä RESULTS FOR: {search_term}")
        print(f"   Average Price: ¬£{avg_price:.2f}")
        print(f"   Sample Count: {len(filtered_prices)}")
        print(f"   Price Range: ¬£{min(filtered_prices):.2f} - ¬£{max(filtered_prices):.2f}")
        print(f"   Median: ¬£{np.median(filtered_prices):.2f}")
        print(f"   Std Dev: ¬£{std_dev:.2f}")
        
        # Save option
        save_option = input("\nSave results? (y/n): ").strip().lower()
        if save_option == 'y':
            filename = f"{search_term.replace(' ', '_')}_prices_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            result_data = [{
                'item_name': search_term,
                'average_price': round(avg_price, 2),
                'median_price': round(np.median(filtered_prices), 2),
                'sample_count': len(filtered_prices),
                'price_range': f"¬£{min(filtered_prices):.2f} - ¬£{max(filtered_prices):.2f}",
                'standard_deviation': round(std_dev, 2),
                'filter_profile_used': filter_profile or 'custom',
                'exclusions_applied': len(scraper.exclude_keywords),
                'all_prices': filtered_prices
            }]
            
            # Choose format
            format_choice = input("Save as (j)son or (e)xcel? ").strip().lower()
            if format_choice.startswith('e'):
                save_results(result_data, filename, 'excel')
            else:
                save_results(result_data, filename, 'json')
            if format_choice.startswith('e'):
                save_results(result_data, filename, 'excel')
            else:
                save_results(result_data, filename, 'json')

def batch_mode(input_file, output_file=None):
    """Batch mode for processing multiple items from file"""
    print(f"\nüìÅ BATCH PROCESSING MODE")
    print(f"Input file: {input_file}")
    print("=" * 50)
    
    # Load items
    items = load_items_from_file(input_file)
    
    if not items:
        print("‚ùå No items loaded from file.")
        return
    
    results = []
    
    print(f"üöÄ Processing {len(items)} items...")
    
    for i, item in enumerate(items, 1):
        item_name = item['name']
        category = item.get('category')
        filter_profile = item.get('filter_profile')
        
        print(f"\nüìä Processing {i}/{len(items)}: {item_name}")
        if filter_profile:
            print(f"   Using filter profile: {filter_profile}")
        print("-" * 50)
        
        # Create scraper with item-specific settings
        scraper = UniversalPriceScraper(filter_profile=filter_profile)
        
        # Scrape prices
        prices = scraper.scrape_item_prices(item_name, category=category)
        
        if not prices:
            result = {
                'item_name': item_name,
                'average_price': None,
                'sample_count': 0,
                'status': 'No data found',
                'filter_profile_used': filter_profile,
                'original_data': item['original_data']
            }
        else:
            # Process results
            filtered_prices, avg_price, std_dev = scraper.filter_outliers(prices)
            
            result = {
                'item_name': item_name,
                'average_price': round(avg_price, 2) if not np.isnan(avg_price) else None,
                'median_price': round(np.median(filtered_prices), 2) if filtered_prices else None,
                'sample_count': len(filtered_prices),
                'price_range': f"¬£{min(filtered_prices):.2f} - ¬£{max(filtered_prices):.2f}" if filtered_prices else "N/A",
                'standard_deviation': round(std_dev, 2) if not np.isnan(std_dev) else None,
                'status': 'Success' if not np.isnan(avg_price) else 'Insufficient data',
                'filter_profile_used': filter_profile or 'default',
                'exclusions_applied': len(scraper.exclude_keywords),
                'all_prices': filtered_prices if len(filtered_prices) <= 50 else filtered_prices[:50],  # Limit stored prices
                'original_data': item['original_data']
            }
            
            if not np.isnan(avg_price):
                print(f"   ‚úÖ Average: ¬£{avg_price:.2f} (n={len(filtered_prices)})")
            else:
                print(f"   ‚ö†Ô∏è Insufficient data")
        
        results.append(result)
        
        # Enhanced safety delay between items in batch mode
        if i < len(items):  # Don't delay after last item
            # Calculate dynamic delay based on success/failure and request count
            base_delay = random.uniform(8, 15)  # Longer base delay for batch processing
            
            # Add extra delay if we've made many requests
            if scraper.request_count > 20:
                extra_delay = (scraper.request_count - 20) * 0.3
                base_delay += extra_delay
                print(f"   ‚è≥ High request count, adding {extra_delay:.1f}s extra delay")
            
            # Add extra delay if last request failed
            if not prices:
                base_delay += random.uniform(5, 10)
                print(f"   ‚è≥ Last request failed, adding extra caution delay")
            
            print(f"   ‚è≥ Safety delay between items: {base_delay:.1f} seconds")
            time.sleep(base_delay)
    
    # Save results
    if not output_file:
        # Generate output filename based on input
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        input_ext = os.path.splitext(input_file)[1].lower()
        output_file = f"{base_name}_results"
        original_format = 'excel' if input_ext in ['.xlsx', '.xls'] else 'json'
    else:
        original_format = 'excel' if output_file.endswith(('.xlsx', '.xls')) else 'json'
    
    save_results(results, output_file, original_format)
    
    # Print summary
    successful = [r for r in results if r['average_price'] is not None]
    print(f"\nüéâ BATCH PROCESSING COMPLETE!")
    print(f"Total items: {len(results)}")
    print(f"Successful: {len(successful)}")
    print(f"Success rate: {len(successful)/len(results)*100:.1f}%")
    
    if successful:
        avg_prices = [r['average_price'] for r in successful]
        print(f"Overall average price: ¬£{np.mean(avg_prices):.2f}")
        print(f"Price range: ¬£{min(avg_prices):.2f} - ¬£{max(avg_prices):.2f}")

def main():
    """Main function with menu system"""
    print("üåê UNIVERSAL EBAY PRICE SCRAPER")
    print("=" * 50)
    print("Choose mode:")
    print("1. Interactive mode (single item searches)")
    print("2. Batch mode (process file)")
    print("3. Create sample input files")
    print("4. View available filter profiles")
    
    choice = input("\nEnter choice (1-4): ").strip()
    
    if choice == '1':
        interactive_mode()
    
    elif choice == '2':
        input_file = input("Enter input file path: ").strip().strip('"')
        output_file = input("Enter output file path (optional, press Enter for auto): ").strip().strip('"')
        output_file = output_file if output_file else None
        batch_mode(input_file, output_file)
    
    elif choice == '3':
        # Create sample files
        print("\nüìÅ Creating sample input files...")
        
        # Sample JSON with filters
        sample_json = [
            {
                "name": "RTX 2080 Super",
                "category": "27386",
                "filter_profile": "graphics_card"
            },
            {
                "name": "iPhone 14 Pro",
                "category": "9355",
                "filter_profile": "phone"
            },
            {
                "name": "MacBook Pro M2",
                "category": "111422",
                "filter_profile": "laptop"
            },
            {
                "name": "Sony PS5 Console",
                "category": "139971",
                "filter_profile": "console"
            },
            {
                "name": "Canon EOS R5",
                "category": "625",
                "filter_profile": "camera"
            }
        ]
        
        with open("sample_items_with_filters.json", "w") as f:
            json.dump(sample_json, f, indent=2)
        
        # Sample Excel with filters
        sample_df = pd.DataFrame([
            {
                "Item Name": "RTX 3080 Ti",
                "Category": "27386",
                "Filter Profile": "graphics_card",
                "Notes": "High-end GPU"
            },
            {
                "Item Name": "Dell XPS 13",
                "Category": "177",
                "Filter Profile": "laptop",
                "Notes": "Ultrabook"
            },
            {
                "Item Name": "Nike Air Jordan 1",
                "Category": "15709",
                "Filter Profile": "clothing",
                "Notes": "Sneakers"
            },
            {
                "Item Name": "BMW E90 Headlight",
                "Category": "6028",
                "Filter Profile": "car_parts",
                "Notes": "Car parts"
            },
        ])
        
        sample_df.to_excel("sample_items_with_filters.xlsx", index=False)
        
        print("‚úÖ Created sample files with filter profiles:")
        print("   - sample_items_with_filters.json")
        print("   - sample_items_with_filters.xlsx")
        print("\nEdit these files with your items and run batch mode!")
    
    elif choice == '4':
        # Show available filter profiles
        print("\nüìã AVAILABLE FILTER PROFILES:")
        print("=" * 50)
        
        for profile_name, keywords in FILTER_PROFILES.items():
            print(f"\nüîç {profile_name.upper()}")
            print(f"   Exclusions: {len(keywords)} keywords")
            print(f"   Examples: {', '.join(keywords[:8])}{'...' if len(keywords) > 8 else ''}")
        
        print(f"\nüí° How to use:")
        print(f"   - In interactive mode: Choose the profile number")
        print(f"   - In Excel files: Add 'Filter Profile' column with profile name")
        print(f"   - In JSON files: Add 'filter_profile' field with profile name")
        
        # Show example usage
        print(f"\nüìù Example JSON with filter:")
        example = {
            "name": "RTX 2080 Super",
            "category": "27386",
            "filter_profile": "graphics_card"
        }
        print(json.dumps(example, indent=2))
    
    else:
        print("Invalid choice.")

if __name__ == "__main__":
    main()
